# Automated Sales Data Cleaning & Reporting Pipeline (Linux-Based)

A Linux-based automated data processing pipeline built using Python and Bash to simulate real-world ETL workflows.

---

## ğŸ“Œ Project Overview

This project simulates a real-world scenario where raw CSV sales data is received daily and needs to be:

- Cleaned
- Processed
- Summarized
- Logged
- Automated using shell scripting

## ğŸ› ï¸ Tech Stack
- Ubuntu (WSL)
- Python 3
- Pandas
- Matplotlib
- Shell Scripting (Bash)
- Virtual Environment
- Git

## ğŸ“‚ Project Structure
sales_pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sales_raw.csv
â”‚   â”œâ”€â”€ sales_cleaned.csv
â”‚   â””â”€â”€ sales_summary.csv
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ clean_data.py
â”‚   â””â”€â”€ generate_report.py
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pipeline.log
â”‚
â”œâ”€â”€ run_pipeline.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

## ğŸ”„ Pipeline Workflow

1. Read raw CSV data
2. Handle missing values
3. Remove duplicates
4. Generate region-wise sales summary
5. Log execution details
6. Automate entire workflow using shell script

## â–¶ï¸ How to Run

Activate virtual environment:
```bash
source venv/bin/activate
```
Run the pipeline

```bash
./run_pipeline.sh
```

## ğŸ“Š Output

- Cleaned dataset
- Region-wise sales summary
- Execution logs

---

## ğŸ—„ SQL Integration

The pipeline integrates SQLite for database-level aggregation and analysis.

### Steps Performed:

1. Cleaned CSV data is loaded into a SQLite database.
2. SQL queries are executed to calculate region-wise total sales.
3. SQL output is exported as `sql_sales_summary.csv`.
4. Entire process is automated within the shell pipeline.

### Example SQL Query Used:

```sql
SELECT region, SUM(amount) AS total_sales
FROM sales
GROUP BY region;

## ğŸ¯ Learning Outcomes

- Data preprocessing using Pandas
- Linux file management
- Shell scripting automation
- Logging and error handling
- Git version control


